#!/usr/bin/env python3
"""
å°æµ‹è¯•é›†è¯„ä¼°è„šæœ¬
åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ¨èç³»ç»Ÿè¯„ä¼°
"""

import os
import json
import numpy as np
import torch
import math
from tqdm import tqdm
import argparse

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'
os.environ['HF_HOME'] = '/root/autodl-tmp/nku/DEALRec-main/models_cache'
os.environ['TRANSFORMERS_CACHE'] = '/root/autodl-tmp/nku/DEALRec-main/models_cache'

from transformers import LlamaForCausalLM, LlamaTokenizer

def setup_arguments():
    parser = argparse.ArgumentParser(description="å°æµ‹è¯•é›†è¯„ä¼°")
    parser.add_argument("--result_file", type=str, 
                       default="./results/games/2023_1024_small.json",
                       help="æ¨ç†ç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset", type=str, default="games", 
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--batch_size", type=int, default=4,
                       help="æ‰¹å¤„ç†å¤§å°")
    return parser.parse_args()

class RecommendationEvaluator:
    def __init__(self, dataset="games", batch_size=4):
        self.dataset = dataset
        self.batch_size = batch_size
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸš€ åˆå§‹åŒ–è¯„ä¼°å™¨...")
        print(f"ğŸ“Š æ•°æ®é›†: {dataset}")
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self._load_model()
        
        # åŠ è½½ç‰©å“æ˜ å°„
        self._load_item_mappings()
        
        # è®¡ç®—ç‰©å“åµŒå…¥
        self._compute_item_embeddings()
    
    def _load_model(self):
        """åŠ è½½LLaMAæ¨¡å‹å’Œtokenizer"""
        print("ğŸ¤– åŠ è½½LLaMAæ¨¡å‹...")
        base_model = "/root/autodl-tmp/nku/DEALRec-main/models_cache/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75"
        
        self.tokenizer = LlamaTokenizer.from_pretrained(base_model)
        self.model = LlamaForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        
        # é…ç½®æ¨¡å‹
        self.model.half()
        self.model.config.pad_token_id = self.tokenizer.pad_token_id = 0
        self.model.config.bos_token_id = 1
        self.model.config.eos_token_id = 2
        self.tokenizer.padding_side = "left"
        self.model.eval()
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _load_item_mappings(self):
        """åŠ è½½ç‰©å“æ˜ å°„å…³ç³»"""
        print("ğŸ“‹ åŠ è½½ç‰©å“æ˜ å°„...")
        try:
            # å°è¯•åŠ è½½åŸå§‹æ˜ å°„æ–‡ä»¶
            mapping_file = f'../../data/{self.dataset}/title_maps.npy'
            if os.path.exists(mapping_file):
                movies = np.load(mapping_file, allow_pickle=True).item()
                self.item_names = list(movies['seqid2title'].values())
                print(f"âœ… ä» {mapping_file} åŠ è½½äº† {len(self.item_names)} ä¸ªç‰©å“")
            else:
                # å¦‚æœæ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»æµ‹è¯•æ•°æ®ä¸­æå–
                print("âš ï¸ æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»æµ‹è¯•æ•°æ®ä¸­æå–ç‰©å“åç§°...")
                self._extract_items_from_test_data()
        except Exception as e:
            print(f"âŒ åŠ è½½æ˜ å°„å¤±è´¥: {e}")
            print("ğŸ“ ä»æµ‹è¯•æ•°æ®ä¸­æå–ç‰©å“åç§°...")
            self._extract_items_from_test_data()
        
        # åˆ›å»ºç‰©å“å­—å…¸
        self.item_dict = {name: idx for idx, name in enumerate(self.item_names)}
        print(f"ğŸ“Š ç‰©å“æ˜ å°„åˆ›å»ºå®Œæˆï¼Œå…± {len(self.item_names)} ä¸ªç‰©å“")
    
    def _extract_items_from_test_data(self):
        """ä»æµ‹è¯•æ•°æ®ä¸­æå–æ‰€æœ‰ç‰©å“åç§°"""
        # ä»å°æµ‹è¯•é›†ä¸­æå–ç‰©å“
        test_file = f'data/{self.dataset}/test/test_small.json'
        if not os.path.exists(test_file):
            test_file = f'data/{self.dataset}/test/test.json'
        
        with open(test_file, 'r') as f:
            test_data = json.load(f)
        
        # æå–æ‰€æœ‰uniqueçš„ç‰©å“åç§°
        all_items = set()
        for item in test_data:
            if isinstance(item['output'], list):
                for out in item['output']:
                    if isinstance(out, str):
                        all_items.add(out.strip('"'))
            elif isinstance(item['output'], str):
                # å°è¯•è§£æå­—ç¬¦ä¸²æ ¼å¼çš„åˆ—è¡¨
                try:
                    import ast
                    parsed = ast.literal_eval(item['output'])
                    if isinstance(parsed, list):
                        for out in parsed:
                            all_items.add(str(out).strip('"'))
                except:
                    all_items.add(item['output'].strip('"'))
        
        self.item_names = list(all_items)
        print(f"ğŸ“ ä»æµ‹è¯•æ•°æ®ä¸­æå–äº† {len(self.item_names)} ä¸ªuniqueç‰©å“")
    
    def _batch_process(self, items, batch_size):
        """æ‰¹å¤„ç†åŠ©æ‰‹å‡½æ•°"""
        for i in range(0, len(items), batch_size):
            yield items[i:i + batch_size]
    
    def _compute_item_embeddings(self):
        """è®¡ç®—æ‰€æœ‰ç‰©å“çš„åµŒå…¥è¡¨ç¤º"""
        print("ğŸ§® è®¡ç®—ç‰©å“åµŒå…¥...")
        item_embeddings = []
        
        with torch.no_grad():
            for batch_items in tqdm(
                self._batch_process(self.item_names, self.batch_size),
                desc="è®¡ç®—ç‰©å“åµŒå…¥",
                total=(len(self.item_names) + self.batch_size - 1) // self.batch_size
            ):
                inputs = self.tokenizer(
                    batch_items, 
                    return_tensors="pt", 
                    padding=True,
                    truncation=True,
                    max_length=128
                ).to(self.device)
                
                outputs = self.model(**inputs, output_hidden_states=True)
                # ä½¿ç”¨æœ€åä¸€å±‚çš„æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€
                hidden_states = outputs.hidden_states[-1][:, -1, :]
                item_embeddings.append(hidden_states.detach().cpu())
        
        self.item_embeddings = torch.cat(item_embeddings, dim=0).to(self.device)
        print(f"âœ… ç‰©å“åµŒå…¥è®¡ç®—å®Œæˆ: {self.item_embeddings.shape}")
    
    def _compute_prediction_embeddings(self, predictions):
        """è®¡ç®—é¢„æµ‹æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤º"""
        print("ğŸ”® è®¡ç®—é¢„æµ‹åµŒå…¥...")
        pred_embeddings = []
        
        with torch.no_grad():
            for batch_preds in tqdm(
                self._batch_process(predictions, self.batch_size),
                desc="è®¡ç®—é¢„æµ‹åµŒå…¥",
                total=(len(predictions) + self.batch_size - 1) // self.batch_size
            ):
                inputs = self.tokenizer(
                    batch_preds,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=256
                ).to(self.device)
                
                outputs = self.model(**inputs, output_hidden_states=True)
                hidden_states = outputs.hidden_states[-1][:, -1, :]
                pred_embeddings.append(hidden_states.detach().cpu())
        
        return torch.cat(pred_embeddings, dim=0).to(self.device)
    
    def _calculate_ndcg(self, ranks, ground_truth, topk):
        """è®¡ç®—NDCG@K"""
        ndcg_sum = 0
        
        for i, gt_items in enumerate(ground_truth):
            dcg = 0
            idcg = 0
            
            # è®¡ç®—DCG
            for j, item in enumerate(gt_items):
                if item in self.item_dict:
                    item_id = self.item_dict[item]
                    rank = ranks[i][item_id].item()
                    if rank < topk:
                        dcg += 1 / math.log2(rank + 2)
                
                # è®¡ç®—IDCG (ç†æƒ³æ’åº)
                if j < topk:
                    idcg += 1 / math.log2(j + 2)
            
            # é¿å…é™¤é›¶
            if idcg > 0:
                ndcg_sum += dcg / idcg
        
        return ndcg_sum / len(ground_truth)
    
    def _calculate_recall(self, ranks, ground_truth, topk):
        """è®¡ç®—Recall@K"""
        recall_sum = 0
        
        for i, gt_items in enumerate(ground_truth):
            hits = 0
            for item in gt_items:
                if item in self.item_dict:
                    item_id = self.item_dict[item]
                    rank = ranks[i][item_id].item()
                    if rank < topk:
                        hits += 1
            
            if len(gt_items) > 0:
                recall_sum += hits / len(gt_items)
        
        return recall_sum / len(ground_truth)
    
    def evaluate(self, result_file):
        """æ‰§è¡Œè¯„ä¼°"""
        print(f"ğŸ“Š å¼€å§‹è¯„ä¼°: {result_file}")
        
        # åŠ è½½æ¨ç†ç»“æœ
        with open(result_file, 'r') as f:
            test_data = json.load(f)
        
        print(f"ğŸ“ åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        # æå–é¢„æµ‹æ–‡æœ¬å’ŒçœŸå®æ ‡ç­¾
        predictions = []
        ground_truth = []
        
        for item in test_data:
            pred = item['predict'].strip('"')
            predictions.append(pred)
            
            # å¤„ç†çœŸå®æ ‡ç­¾
            gt = item['output']
            if isinstance(gt, str):
                try:
                    import ast
                    gt = ast.literal_eval(gt)
                except:
                    gt = [gt]
            if isinstance(gt, list):
                gt = [str(x).strip('"') for x in gt]
            else:
                gt = [str(gt).strip('"')]
            
            ground_truth.append(gt)
        
        # è®¡ç®—é¢„æµ‹åµŒå…¥
        pred_embeddings = self._compute_prediction_embeddings(predictions)
        
        # è®¡ç®—è·ç¦»å’Œæ’å
        print("ğŸ“ è®¡ç®—ç›¸ä¼¼åº¦è·ç¦»...")
        distances = torch.cdist(pred_embeddings, self.item_embeddings, p=2)
        ranks = distances.argsort(dim=-1).argsort(dim=-1)
        
        # åœ¨ä¸åŒKå€¼ä¸‹è¯„ä¼°
        topk_list = [5, 10, 20, 50]
        results = {}
        
        print("ğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        for topk in topk_list:
            ndcg = self._calculate_ndcg(ranks, ground_truth, topk)
            recall = self._calculate_recall(ranks, ground_truth, topk)
            
            results[f'NDCG@{topk}'] = round(ndcg, 4)
            results[f'Recall@{topk}'] = round(recall, 4)
        
        return results
    
    def print_results(self, results):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°ç»“æœ")
        print("="*60)
        
        # æ‰“å°è¡¨æ ¼
        print(f"{'æŒ‡æ ‡':<12} {'@5':<8} {'@10':<8} {'@20':<8} {'@50':<8}")
        print("-"*50)
        
        ndcg_values = [results[f'NDCG@{k}'] for k in [5, 10, 20, 50]]
        recall_values = [results[f'Recall@{k}'] for k in [5, 10, 20, 50]]
        
        print(f"{'NDCG':<12} {ndcg_values[0]:<8} {ndcg_values[1]:<8} {ndcg_values[2]:<8} {ndcg_values[3]:<8}")
        print(f"{'Recall':<12} {recall_values[0]:<8} {recall_values[1]:<8} {recall_values[2]:<8} {recall_values[3]:<8}")
        
        print("\nğŸ’¡ æŒ‡æ ‡è§£é‡Š:")
        print("  NDCG: è€ƒè™‘æ’åºä½ç½®çš„å‡†ç¡®ç‡ (è¶Šé«˜è¶Šå¥½)")
        print("  Recall: å¬å›ç‡ï¼Œæ‰¾åˆ°çš„ç›¸å…³ç‰©å“æ¯”ä¾‹ (è¶Šé«˜è¶Šå¥½)")

def main():
    args = setup_arguments()
    
    # æ£€æŸ¥ç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.result_file):
        print(f"âŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {args.result_file}")
        print("è¯·å…ˆè¿è¡Œ inference_small.py ç”Ÿæˆæ¨ç†ç»“æœ")
        return
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = RecommendationEvaluator(
        dataset=args.dataset,
        batch_size=args.batch_size
    )
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.evaluate(args.result_file)
    
    # æ‰“å°ç»“æœ
    evaluator.print_results(results)
    
    # ä¿å­˜ç»“æœ
    output_file = args.result_file.replace('.json', '_evaluation.json')
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main() 