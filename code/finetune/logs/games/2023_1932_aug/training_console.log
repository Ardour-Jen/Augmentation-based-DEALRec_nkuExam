NVIDIA GeForce RTX 4090

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
CUDA SETUP: CUDA runtime path found: /root/miniconda3/envs/DEALRec/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.9
CUDA SETUP: Detected CUDA version 118
CUDA SETUP: Loading binary /root/miniconda3/envs/DEALRec/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...
Training Alpaca-LoRA model with params:
base_model: /root/autodl-tmp/nku/DEALRec-main/models_cache/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75
train_data_path: code/finetune/data/games/train/train-1932.json
val_data_path: code/finetune/data/games/valid/valid-365.json
sample: -1
seed: 2023
output_dir: code/finetune/model/games/2023_1932_aug
batch_size: 128
micro_batch_size: 16
num_epochs: 30
learning_rate: 0.0001
cutoff_len: 512
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj']
train_on_inputs: True
group_by_length: True
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: None

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.42s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/miniconda3/envs/DEALRec/lib/python3.8/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Loading training data from: code/finetune/data/games/train/train-1932.json
Loading validation data from: code/finetune/data/games/valid/valid-365.json
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.12433454005023165
  0%|          | 0/450 [00:00<?, ?it/s]/root/miniconda3/envs/DEALRec/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/450 [00:25<3:10:31, 25.46s/it]                                                   0%|          | 1/450 [00:25<3:10:31, 25.46s/it]  0%|          | 2/450 [00:40<2:26:24, 19.61s/it]  1%|          | 3/450 [00:54<2:05:06, 16.79s/it]  1%|          | 4/450 [01:21<2:33:57, 20.71s/it]  1%|          | 5/450 [01:36<2:20:12, 18.90s/it]                                                   1%|          | 5/450 [01:36<2:20:12, 18.90s/it]