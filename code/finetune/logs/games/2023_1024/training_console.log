NVIDIA GeForce RTX 4090

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
CUDA SETUP: CUDA runtime path found: /root/miniconda3/envs/DEALRec/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.9
CUDA SETUP: Detected CUDA version 118
CUDA SETUP: Loading binary /root/miniconda3/envs/DEALRec/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...
Training Alpaca-LoRA model with params:
base_model: /root/autodl-tmp/nku/DEALRec-main/models_cache/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75
train_data_path: ./data/games/train/train-1024.json
val_data_path: ./data/games/valid/valid-1024.json
sample: 1024
seed: 2023
output_dir: ./model/games/2023_1024
batch_size: 128
micro_batch_size: 16
num_epochs: 30
learning_rate: 0.0001
cutoff_len: 512
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj']
train_on_inputs: True
group_by_length: True
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: None

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/miniconda3/envs/DEALRec/lib/python3.8/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Loading training data from: ./data/games/train/train-1024.json
Loading validation data from: ./data/games/valid/valid-1024.json
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.12433454005023165
  0%|          | 0/240 [00:00<?, ?it/s]/root/miniconda3/envs/DEALRec/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/240 [00:23<1:34:17, 23.67s/it]                                                   0%|          | 1/240 [00:23<1:34:17, 23.67s/it]  1%|          | 2/240 [00:37<1:10:55, 17.88s/it]  1%|â–         | 3/240 [01:00<1:19:04, 20.02s/it]  2%|â–         | 4/240 [01:13<1:08:59, 17.54s/it]  2%|â–         | 5/240 [01:37<1:16:59, 19.66s/it]                                                   2%|â–         | 5/240 [01:37<1:16:59, 19.66s/it]  2%|â–Ž         | 6/240 [01:51<1:09:11, 17.74s/it]  3%|â–Ž         | 7/240 [02:15<1:17:45, 20.02s/it]  3%|â–Ž         | 8/240 [02:29<1:09:50, 18.06s/it]  4%|â–         | 9/240 [02:53<1:16:05, 19.76s/it]  4%|â–         | 10/240 [03:07<1:08:40, 17.92s/it]                                                    4%|â–         | 10/240 [03:07<1:08:40, 17.92s/it]  5%|â–         | 11/240 [03:29<1:13:41, 19.31s/it]  5%|â–Œ         | 12/240 [03:43<1:07:04, 17.65s/it]  5%|â–Œ         | 13/240 [04:08<1:15:47, 20.03s/it]  6%|â–Œ         | 14/240 [04:23<1:08:40, 18.23s/it]  6%|â–‹         | 15/240 [04:45<1:13:06, 19.50s/it]                                                    6%|â–‹         | 15/240 [04:45<1:13:06, 19.50s/it]  7%|â–‹         | 16/240 [04:59<1:06:19, 17.76s/it]  7%|â–‹         | 17/240 [05:23<1:13:24, 19.75s/it]  8%|â–Š         | 18/240 [05:37<1:06:30, 17.98s/it]  8%|â–Š         | 19/240 [06:00<1:12:09, 19.59s/it]  8%|â–Š         | 20/240 [06:14<1:05:35, 17.89s/it]                                                    8%|â–Š         | 20/240 [06:14<1:05:35, 17.89s/it]  9%|â–‰         | 21/240 [06:37<1:10:51, 19.41s/it]  9%|â–‰         | 22/240 [06:51<1:04:36, 17.78s/it] 10%|â–‰         | 23/240 [07:14<1:09:56, 19.34s/it] 10%|â–ˆ         | 24/240 [07:28<1:03:43, 17.70s/it] 10%|â–ˆ         | 25/240 [07:52<1:09:44, 19.46s/it]                                                   10%|â–ˆ         | 25/240 [07:52<1:09:44, 19.46s/it] 11%|â–ˆ         | 26/240 [08:05<1:03:29, 17.80s/it] 11%|â–ˆâ–        | 27/240 [08:28<1:08:40, 19.35s/it] 12%|â–ˆâ–        | 28/240 [08:42<1:02:26, 17.67s/it] 12%|â–ˆâ–        | 29/240 [09:06<1:08:24, 19.45s/it] 12%|â–ˆâ–Ž        | 30/240 [09:20<1:02:12, 17.78s/it]                                                   12%|â–ˆâ–Ž        | 30/240 [09:20<1:02:12, 17.78s/it] 13%|â–ˆâ–Ž        | 31/240 [09:43<1:07:50, 19.48s/it] 13%|â–ˆâ–Ž        | 32/240 [09:57<1:01:34, 17.76s/it] 14%|â–ˆâ–        | 33/240 [10:20<1:06:39, 19.32s/it] 14%|â–ˆâ–        | 34/240 [10:34<1:00:41, 17.68s/it] 15%|â–ˆâ–        | 35/240 [10:57<1:05:47, 19.26s/it]                                                   15%|â–ˆâ–        | 35/240 [10:57<1:05:47, 19.26s/it] 15%|â–ˆâ–Œ        | 36/240 [11:10<59:54, 17.62s/it]   15%|â–ˆâ–Œ        | 37/240 [11:34<1:05:51, 19.47s/it] 16%|â–ˆâ–Œ        | 38/240 [11:48<59:59, 17.82s/it]   16%|â–ˆâ–‹        | 39/240 [12:11<1:05:08, 19.45s/it] 17%|â–ˆâ–‹        | 40/240 [12:25<59:05, 17.73s/it]                                                   17%|â–ˆâ–‹        | 40/240 [12:25<59:05, 17.73s/it] 17%|â–ˆâ–‹        | 41/240 [12:48<1:04:03, 19.31s/it] 18%|â–ˆâ–Š        | 42/240 [13:02<58:17, 17.66s/it]   18%|â–ˆâ–Š        | 43/240 [13:27<1:04:59, 19.80s/it] 18%|â–ˆâ–Š        | 44/240 [13:41<58:56, 18.04s/it]   19%|â–ˆâ–‰        | 45/240 [14:04<1:03:37, 19.58s/it]                                                   19%|â–ˆâ–‰        | 45/240 [14:04<1:03:37, 19.58s/it] 19%|â–ˆâ–‰        | 46/240 [14:18<57:47, 17.87s/it]   20%|â–ˆâ–‰        | 47/240 [14:41<1:02:14, 19.35s/it] 20%|â–ˆâ–ˆ        | 48/240 [14:54<56:30, 17.66s/it]   20%|â–ˆâ–ˆ        | 49/240 [15:17<1:01:24, 19.29s/it] 21%|â–ˆâ–ˆ        | 50/240 [15:31<55:53, 17.65s/it]                                                   21%|â–ˆâ–ˆ        | 50/240 [15:31<55:53, 17.65s/it]ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 0.12, Step: 1, Train Loss: N/A, Eval Loss: N/A, LR: 5e-06
{'loss': 2.4355, 'learning_rate': 5e-06, 'epoch': 0.12}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 0.62, Step: 5, Train Loss: N/A, Eval Loss: N/A, LR: 2.5e-05
{'loss': 2.6336, 'learning_rate': 2.5e-05, 'epoch': 0.62}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 1.25, Step: 10, Train Loss: N/A, Eval Loss: N/A, LR: 5e-05
{'loss': 2.6107, 'learning_rate': 5e-05, 'epoch': 1.25}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 1.88, Step: 15, Train Loss: N/A, Eval Loss: N/A, LR: 7.500000000000001e-05
{'loss': 2.4339, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.88}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 2.50, Step: 20, Train Loss: N/A, Eval Loss: N/A, LR: 0.0001
{'loss': 2.2647, 'learning_rate': 0.0001, 'epoch': 2.5}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 3.12, Step: 25, Train Loss: N/A, Eval Loss: N/A, LR: 9.772727272727274e-05
{'loss': 1.8633, 'learning_rate': 9.772727272727274e-05, 'epoch': 3.12}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 3.75, Step: 30, Train Loss: N/A, Eval Loss: N/A, LR: 9.545454545454546e-05
{'loss': 1.4359, 'learning_rate': 9.545454545454546e-05, 'epoch': 3.75}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 4.38, Step: 35, Train Loss: N/A, Eval Loss: N/A, LR: 9.318181818181818e-05
{'loss': 1.1569, 'learning_rate': 9.318181818181818e-05, 'epoch': 4.38}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 5.00, Step: 40, Train Loss: N/A, Eval Loss: N/A, LR: 9.090909090909092e-05
{'loss': 0.9762, 'learning_rate': 9.090909090909092e-05, 'epoch': 5.0}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 5.62, Step: 45, Train Loss: N/A, Eval Loss: N/A, LR: 8.863636363636364e-05
{'loss': 0.9493, 'learning_rate': 8.863636363636364e-05, 'epoch': 5.62}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 6.25, Step: 50, Train Loss: N/A, Eval Loss: N/A, LR: 8.636363636363637e-05
{'loss': 0.8271, 'learning_rate': 8.636363636363637e-05, 'epoch': 6.25}

  0%|          | 0/12 [00:00<?, ?it/s][A
 17%|â–ˆâ–‹        | 2/12 [00:00<00:04,  2.17it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.17it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:08,  1.04s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:05<00:08,  1.17s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:06<00:07,  1.25s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:07<00:06,  1.22s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:08<00:04,  1.18s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:09<00:03,  1.10s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:10<00:02,  1.01s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:11<00:00,  1.10it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.13it/s][A                                                
                                               [A 21%|â–ˆâ–ˆ        | 50/240 [15:44<55:53, 17.65s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.13it/s][A
                                               [A 21%|â–ˆâ–ˆâ–       | 51/240 [16:07<1:13:00, 23.18s/it] 22%|â–ˆâ–ˆâ–       | 52/240 [16:21<1:03:54, 20.39s/it] 22%|â–ˆâ–ˆâ–       | 53/240 [16:44<1:06:13, 21.25s/it] 22%|â–ˆâ–ˆâ–Ž       | 54/240 [16:58<58:56, 19.02s/it]   23%|â–ˆâ–ˆâ–Ž       | 55/240 [17:22<1:02:46, 20.36s/it]                                                   23%|â–ˆâ–ˆâ–Ž       | 55/240 [17:22<1:02:46, 20.36s/it] 23%|â–ˆâ–ˆâ–Ž       | 56/240 [17:35<56:21, 18.38s/it]   24%|â–ˆâ–ˆâ–       | 57/240 [17:59<1:00:39, 19.89s/it] 24%|â–ˆâ–ˆâ–       | 58/240 [18:12<54:37, 18.01s/it]   25%|â–ˆâ–ˆâ–       | 59/240 [18:35<58:20, 19.34s/it] 25%|â–ˆâ–ˆâ–Œ       | 60/240 [18:49<53:01, 17.68s/it]                                                 25%|â–ˆâ–ˆâ–Œ       | 60/240 [18:49<53:01, 17.68s/it] 25%|â–ˆâ–ˆâ–Œ       | 61/240 [19:12<57:43, 19.35s/it] 26%|â–ˆâ–ˆâ–Œ       | 62/240 [19:26<52:37, 17.74s/it] 26%|â–ˆâ–ˆâ–‹       | 63/240 [19:50<58:19, 19.77s/it] 27%|â–ˆâ–ˆâ–‹       | 64/240 [20:04<52:57, 18.05s/it] 27%|â–ˆâ–ˆâ–‹       | 65/240 [20:27<56:43, 19.45s/it]                                                 27%|â–ˆâ–ˆâ–‹       | 65/240 [20:27<56:43, 19.45s/it] 28%|â–ˆâ–ˆâ–Š       | 66/240 [20:41<51:10, 17.65s/it] 28%|â–ˆâ–ˆâ–Š       | 67/240 [21:04<55:30, 19.25s/it] 28%|â–ˆâ–ˆâ–Š       | 68/240 [21:17<50:33, 17.64s/it] 29%|â–ˆâ–ˆâ–‰       | 69/240 [21:42<55:52, 19.61s/it] 29%|â–ˆâ–ˆâ–‰       | 70/240 [21:56<50:49, 17.94s/it]                                                 29%|â–ˆâ–ˆâ–‰       | 70/240 [21:56<50:49, 17.94s/it] 30%|â–ˆâ–ˆâ–‰       | 71/240 [22:19<55:10, 19.59s/it] 30%|â–ˆâ–ˆâ–ˆ       | 72/240 [22:33<50:07, 17.90s/it] 30%|â–ˆâ–ˆâ–ˆ       | 73/240 [22:57<54:23, 19.54s/it] 31%|â–ˆâ–ˆâ–ˆ       | 74/240 [23:10<49:08, 17.76s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 75/240 [23:33<53:08, 19.33s/it]                                                 31%|â–ˆâ–ˆâ–ˆâ–      | 75/240 [23:33<53:08, 19.33s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 76/240 [23:47<48:27, 17.73s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 77/240 [24:11<53:01, 19.52s/it] 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 78/240 [24:25<48:12, 17.86s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 79/240 [24:48<52:04, 19.41s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 80/240 [25:02<47:19, 17.75s/it]                                                 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 80/240 [25:02<47:19, 17.75s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 81/240 [25:25<51:39, 19.49s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 82/240 [25:39<46:45, 17.76s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 83/240 [26:03<51:41, 19.75s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 84/240 [26:17<46:54, 18.04s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/240 [26:41<50:46, 19.66s/it]                                                 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/240 [26:41<50:46, 19.66s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 86/240 [26:55<45:59, 17.92s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 87/240 [27:17<49:13, 19.30s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 88/240 [27:31<44:36, 17.61s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 89/240 [27:54<48:17, 19.19s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 90/240 [28:08<43:56, 17.58s/it]                                                 38%|â–ˆâ–ˆâ–ˆâ–Š      | 90/240 [28:08<43:56, 17.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 91/240 [28:31<48:21, 19.47s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 92/240 [28:46<44:01, 17.85s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 93/240 [29:08<47:12, 19.27s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 94/240 [29:22<42:54, 17.63s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 95/240 [29:46<47:06, 19.50s/it]                                                 40%|â–ˆâ–ˆâ–ˆâ–‰      | 95/240 [29:46<47:06, 19.50s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 96/240 [29:59<42:37, 17.76s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 97/240 [30:23<46:17, 19.42s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 98/240 [30:36<41:53, 17.70s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 99/240 [31:00<45:47, 19.49s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/240 [31:14<41:30, 17.79s/it]                                                  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/240 [31:14<41:30, 17.79s/it]ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 6.25, Step: 50, Train Loss: N/A, Eval Loss: 1.0628026723861694, LR: N/A
{'eval_loss': 1.0628026723861694, 'eval_runtime': 12.7505, 'eval_samples_per_second': 15.058, 'eval_steps_per_second': 0.941, 'epoch': 6.25}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 6.88, Step: 55, Train Loss: N/A, Eval Loss: N/A, LR: 8.40909090909091e-05
{'loss': 0.7771, 'learning_rate': 8.40909090909091e-05, 'epoch': 6.88}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 7.50, Step: 60, Train Loss: N/A, Eval Loss: N/A, LR: 8.181818181818183e-05
{'loss': 0.6352, 'learning_rate': 8.181818181818183e-05, 'epoch': 7.5}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 8.12, Step: 65, Train Loss: N/A, Eval Loss: N/A, LR: 7.954545454545455e-05
{'loss': 0.7279, 'learning_rate': 7.954545454545455e-05, 'epoch': 8.12}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 8.75, Step: 70, Train Loss: N/A, Eval Loss: N/A, LR: 7.727272727272727e-05
{'loss': 0.6254, 'learning_rate': 7.727272727272727e-05, 'epoch': 8.75}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 9.38, Step: 75, Train Loss: N/A, Eval Loss: N/A, LR: 7.500000000000001e-05
{'loss': 0.7037, 'learning_rate': 7.500000000000001e-05, 'epoch': 9.38}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 10.00, Step: 80, Train Loss: N/A, Eval Loss: N/A, LR: 7.272727272727273e-05
{'loss': 0.6261, 'learning_rate': 7.272727272727273e-05, 'epoch': 10.0}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 10.62, Step: 85, Train Loss: N/A, Eval Loss: N/A, LR: 7.045454545454546e-05
{'loss': 0.69, 'learning_rate': 7.045454545454546e-05, 'epoch': 10.62}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 11.25, Step: 90, Train Loss: N/A, Eval Loss: N/A, LR: 6.818181818181818e-05
{'loss': 0.6062, 'learning_rate': 6.818181818181818e-05, 'epoch': 11.25}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 11.88, Step: 95, Train Loss: N/A, Eval Loss: N/A, LR: 6.59090909090909e-05
{'loss': 0.7011, 'learning_rate': 6.59090909090909e-05, 'epoch': 11.88}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 12.50, Step: 100, Train Loss: N/A, Eval Loss: N/A, LR: 6.363636363636364e-05
{'loss': 0.6065, 'learning_rate': 6.363636363636364e-05, 'epoch': 12.5}

  0%|          | 0/12 [00:00<?, ?it/s][A
 17%|â–ˆâ–‹        | 2/12 [00:00<00:04,  2.17it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.17it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:08,  1.04s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:05<00:08,  1.17s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:06<00:07,  1.25s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:07<00:06,  1.22s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:08<00:04,  1.18s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:09<00:03,  1.10s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:10<00:02,  1.02s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:11<00:00,  1.09it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.12it/s][A                                                 
                                               [A 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/240 [31:27<41:30, 17.79s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.12it/s][A
                                               [A 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 101/240 [31:50<53:42, 23.18s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 102/240 [32:04<46:52, 20.38s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 103/240 [32:27<48:23, 21.20s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 104/240 [32:40<43:02, 18.99s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 105/240 [33:04<45:34, 20.26s/it]                                                  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 105/240 [33:04<45:34, 20.26s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/240 [33:18<40:58, 18.35s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 107/240 [33:41<43:53, 19.80s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 108/240 [33:55<39:40, 18.03s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 109/240 [34:18<42:45, 19.59s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 110/240 [34:32<38:36, 17.82s/it]                                                  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 110/240 [34:32<38:36, 17.82s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 111/240 [34:55<42:11, 19.63s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 112/240 [35:09<38:00, 17.81s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 113/240 [35:32<41:05, 19.41s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 114/240 [35:46<37:16, 17.75s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 115/240 [36:10<40:50, 19.60s/it]                                                  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 115/240 [36:10<40:50, 19.60s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 116/240 [36:24<36:56, 17.88s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 117/240 [36:47<40:05, 19.56s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 118/240 [37:01<36:19, 17.86s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 119/240 [37:24<38:55, 19.30s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 120/240 [37:37<35:11, 17.59s/it]                                                  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 120/240 [37:37<35:11, 17.59s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 121/240 [38:01<38:39, 19.49s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 122/240 [38:15<34:55, 17.76s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 123/240 [38:38<37:38, 19.31s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 124/240 [38:52<34:16, 17.72s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 125/240 [39:16<37:16, 19.44s/it]                                                  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 125/240 [39:16<37:16, 19.44s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 126/240 [39:29<33:46, 17.78s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 127/240 [39:52<36:25, 19.34s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 128/240 [40:06<32:56, 17.65s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 129/240 [40:30<36:17, 19.62s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 130/240 [40:44<32:47, 17.88s/it]                                                  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 130/240 [40:44<32:47, 17.88s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 131/240 [41:07<35:18, 19.43s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 132/240 [41:21<31:55, 17.73s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 133/240 [41:44<34:27, 19.32s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 134/240 [41:58<31:09, 17.64s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 135/240 [42:21<33:39, 19.23s/it]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 135/240 [42:21<33:39, 19.23s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 136/240 [42:35<30:33, 17.63s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 137/240 [42:57<32:59, 19.22s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 138/240 [43:11<29:52, 17.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 139/240 [43:35<32:41, 19.42s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 140/240 [43:49<29:39, 17.80s/it]                                                  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 140/240 [43:49<29:39, 17.80s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 141/240 [44:12<31:48, 19.28s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 142/240 [44:25<28:48, 17.64s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 143/240 [44:50<31:48, 19.67s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 144/240 [45:04<28:38, 17.90s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 145/240 [45:27<30:51, 19.49s/it]                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 145/240 [45:27<30:51, 19.49s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 146/240 [45:41<27:50, 17.77s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 147/240 [46:04<30:02, 19.38s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 148/240 [46:18<27:11, 17.73s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 149/240 [46:41<29:23, 19.38s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 150/240 [46:55<26:29, 17.66s/it]                                                  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 150/240 [46:55<26:29, 17.66s/it]ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 12.50, Step: 100, Train Loss: N/A, Eval Loss: 0.8700583577156067, LR: N/A
{'eval_loss': 0.8700583577156067, 'eval_runtime': 12.7841, 'eval_samples_per_second': 15.019, 'eval_steps_per_second': 0.939, 'epoch': 12.5}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 13.12, Step: 105, Train Loss: N/A, Eval Loss: N/A, LR: 6.136363636363636e-05
{'loss': 0.6746, 'learning_rate': 6.136363636363636e-05, 'epoch': 13.12}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 13.75, Step: 110, Train Loss: N/A, Eval Loss: N/A, LR: 5.90909090909091e-05
{'loss': 0.6117, 'learning_rate': 5.90909090909091e-05, 'epoch': 13.75}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 14.38, Step: 115, Train Loss: N/A, Eval Loss: N/A, LR: 5.6818181818181825e-05
{'loss': 0.6704, 'learning_rate': 5.6818181818181825e-05, 'epoch': 14.38}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 15.00, Step: 120, Train Loss: N/A, Eval Loss: N/A, LR: 5.4545454545454546e-05
{'loss': 0.598, 'learning_rate': 5.4545454545454546e-05, 'epoch': 15.0}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 15.62, Step: 125, Train Loss: N/A, Eval Loss: N/A, LR: 5.2272727272727274e-05
{'loss': 0.6725, 'learning_rate': 5.2272727272727274e-05, 'epoch': 15.62}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 16.25, Step: 130, Train Loss: N/A, Eval Loss: N/A, LR: 5e-05
{'loss': 0.6006, 'learning_rate': 5e-05, 'epoch': 16.25}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 16.88, Step: 135, Train Loss: N/A, Eval Loss: N/A, LR: 4.772727272727273e-05
{'loss': 0.6612, 'learning_rate': 4.772727272727273e-05, 'epoch': 16.88}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 17.50, Step: 140, Train Loss: N/A, Eval Loss: N/A, LR: 4.545454545454546e-05
{'loss': 0.5875, 'learning_rate': 4.545454545454546e-05, 'epoch': 17.5}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 18.12, Step: 145, Train Loss: N/A, Eval Loss: N/A, LR: 4.318181818181819e-05
{'loss': 0.6595, 'learning_rate': 4.318181818181819e-05, 'epoch': 18.12}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 18.75, Step: 150, Train Loss: N/A, Eval Loss: N/A, LR: 4.0909090909090915e-05
{'loss': 0.5842, 'learning_rate': 4.0909090909090915e-05, 'epoch': 18.75}

  0%|          | 0/12 [00:00<?, ?it/s][A
 17%|â–ˆâ–‹        | 2/12 [00:00<00:04,  2.15it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.17it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:08,  1.05s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:05<00:08,  1.17s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:06<00:07,  1.25s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:07<00:06,  1.22s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:08<00:04,  1.18s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:09<00:03,  1.10s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:10<00:02,  1.02s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:11<00:00,  1.09it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.12it/s][A                                                 
                                               [A 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 150/240 [47:07<26:29, 17.66s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.12it/s][A
                                               [A 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 151/240 [47:31<34:31, 23.27s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 152/240 [47:45<30:02, 20.49s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 153/240 [48:08<30:51, 21.29s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 154/240 [48:22<27:18, 19.05s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 155/240 [48:45<28:31, 20.14s/it]                                                  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 155/240 [48:45<28:31, 20.14s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 156/240 [48:58<25:33, 18.26s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 157/240 [49:22<27:35, 19.95s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 158/240 [49:36<24:46, 18.13s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 159/240 [50:00<26:39, 19.75s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 160/240 [50:14<23:57, 17.97s/it]                                                  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 160/240 [50:14<23:57, 17.97s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 161/240 [50:37<25:42, 19.53s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 162/240 [50:51<23:11, 17.84s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 163/240 [51:14<25:03, 19.53s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 164/240 [51:28<22:35, 17.83s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 165/240 [51:51<24:19, 19.47s/it]                                                  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 165/240 [51:51<24:19, 19.47s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 166/240 [52:05<21:53, 17.75s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 167/240 [52:28<23:40, 19.46s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 168/240 [52:42<21:16, 17.73s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 169/240 [53:05<22:58, 19.42s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 170/240 [53:20<20:46, 17.81s/it]                                                  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 170/240 [53:20<20:46, 17.81s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 171/240 [53:44<22:47, 19.81s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 172/240 [53:58<20:24, 18.01s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 173/240 [54:21<21:48, 19.53s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 174/240 [54:35<19:33, 17.78s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 175/240 [54:58<21:00, 19.39s/it]                                                  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 175/240 [54:58<21:00, 19.39s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 176/240 [55:12<18:52, 17.70s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 177/240 [55:36<20:36, 19.63s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 178/240 [55:49<18:28, 17.87s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 179/240 [56:13<19:50, 19.52s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 180/240 [56:27<17:46, 17.78s/it]                                                  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 180/240 [56:27<17:46, 17.78s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 181/240 [56:49<18:57, 19.28s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 182/240 [57:03<17:02, 17.64s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 183/240 [57:26<18:23, 19.37s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 184/240 [57:41<16:36, 17.79s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 185/240 [58:03<17:41, 19.31s/it]                                                  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 185/240 [58:03<17:41, 19.31s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 186/240 [58:17<15:54, 17.67s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 187/240 [58:40<16:58, 19.22s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 188/240 [58:54<15:13, 17.57s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 189/240 [59:17<16:22, 19.26s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 190/240 [59:31<14:40, 17.61s/it]                                                  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 190/240 [59:31<14:40, 17.61s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 191/240 [59:56<16:11, 19.83s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 192/240 [1:00:10<14:28, 18.10s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 193/240 [1:00:33<15:15, 19.48s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 194/240 [1:00:46<13:33, 17.69s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 195/240 [1:01:10<14:39, 19.55s/it]                                                    81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 195/240 [1:01:10<14:39, 19.55s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 196/240 [1:01:24<13:02, 17.79s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 197/240 [1:01:47<14:00, 19.55s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 198/240 [1:02:01<12:32, 17.93s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 199/240 [1:02:25<13:18, 19.48s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 200/240 [1:02:39<11:52, 17.81s/it]                                                    83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 200/240 [1:02:39<11:52, 17.81s/it]ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 18.75, Step: 150, Train Loss: N/A, Eval Loss: 0.8469192385673523, LR: N/A
{'eval_loss': 0.8469192385673523, 'eval_runtime': 12.805, 'eval_samples_per_second': 14.994, 'eval_steps_per_second': 0.937, 'epoch': 18.75}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 19.38, Step: 155, Train Loss: N/A, Eval Loss: N/A, LR: 3.8636363636363636e-05
{'loss': 0.6546, 'learning_rate': 3.8636363636363636e-05, 'epoch': 19.38}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 20.00, Step: 160, Train Loss: N/A, Eval Loss: N/A, LR: 3.6363636363636364e-05
{'loss': 0.5942, 'learning_rate': 3.6363636363636364e-05, 'epoch': 20.0}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 20.62, Step: 165, Train Loss: N/A, Eval Loss: N/A, LR: 3.409090909090909e-05
{'loss': 0.6577, 'learning_rate': 3.409090909090909e-05, 'epoch': 20.62}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 21.25, Step: 170, Train Loss: N/A, Eval Loss: N/A, LR: 3.181818181818182e-05
{'loss': 0.5837, 'learning_rate': 3.181818181818182e-05, 'epoch': 21.25}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 21.88, Step: 175, Train Loss: N/A, Eval Loss: N/A, LR: 2.954545454545455e-05
{'loss': 0.6521, 'learning_rate': 2.954545454545455e-05, 'epoch': 21.88}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 22.50, Step: 180, Train Loss: N/A, Eval Loss: N/A, LR: 2.7272727272727273e-05
{'loss': 0.5671, 'learning_rate': 2.7272727272727273e-05, 'epoch': 22.5}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 23.12, Step: 185, Train Loss: N/A, Eval Loss: N/A, LR: 2.5e-05
{'loss': 0.6548, 'learning_rate': 2.5e-05, 'epoch': 23.12}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 23.75, Step: 190, Train Loss: N/A, Eval Loss: N/A, LR: 2.272727272727273e-05
{'loss': 0.5661, 'learning_rate': 2.272727272727273e-05, 'epoch': 23.75}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 24.38, Step: 195, Train Loss: N/A, Eval Loss: N/A, LR: 2.0454545454545457e-05
{'loss': 0.6504, 'learning_rate': 2.0454545454545457e-05, 'epoch': 24.38}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 25.00, Step: 200, Train Loss: N/A, Eval Loss: N/A, LR: 1.8181818181818182e-05
{'loss': 0.5835, 'learning_rate': 1.8181818181818182e-05, 'epoch': 25.0}

  0%|          | 0/12 [00:00<?, ?it/s][A
 17%|â–ˆâ–‹        | 2/12 [00:00<00:04,  2.14it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.16it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:08,  1.05s/it][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:05<00:08,  1.18s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:06<00:07,  1.25s/it][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:07<00:06,  1.22s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:08<00:04,  1.19s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:09<00:03,  1.11s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:10<00:02,  1.02s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:11<00:00,  1.09it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.12it/s][A                                                   
                                               [A 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 200/240 [1:02:51<11:52, 17.81s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.12it/s][A
                                               [A/root/miniconda3/envs/DEALRec/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 201/240 [1:03:16<15:21, 23.64s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 202/240 [1:03:30<13:08, 20.74s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 203/240 [1:03:53<13:17, 21.54s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 204/240 [1:04:07<11:31, 19.21s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 205/240 [1:04:30<11:50, 20.30s/it]                                                    85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 205/240 [1:04:30<11:50, 20.30s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 206/240 [1:04:44<10:25, 18.40s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 207/240 [1:05:06<10:50, 19.71s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 208/240 [1:05:20<09:32, 17.90s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 209/240 [1:05:44<10:10, 19.69s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 210/240 [1:05:58<08:58, 17.95s/it]                                                    88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 210/240 [1:05:58<08:58, 17.95s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 211/240 [1:06:21<09:28, 19.59s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 212/240 [1:06:35<08:20, 17.88s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 213/240 [1:06:58<08:42, 19.37s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 214/240 [1:07:12<07:40, 17.71s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 215/240 [1:07:35<08:05, 19.41s/it]                                                    90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 215/240 [1:07:35<08:05, 19.41s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 216/240 [1:07:49<07:05, 17.73s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 217/240 [1:08:12<07:25, 19.39s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 218/240 [1:08:26<06:30, 17.75s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 219/240 [1:08:49<06:40, 19.09s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 220/240 [1:09:02<05:49, 17.47s/it]                                                    92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 220/240 [1:09:02<05:49, 17.47s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 221/240 [1:09:27<06:12, 19.59s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 222/240 [1:09:41<05:22, 17.90s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 223/240 [1:10:04<05:30, 19.46s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 224/240 [1:10:18<04:45, 17.82s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 225/240 [1:10:40<04:49, 19.29s/it]                                                    94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 225/240 [1:10:40<04:49, 19.29s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 226/240 [1:10:54<04:06, 17.61s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 227/240 [1:11:17<04:10, 19.28s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 228/240 [1:11:31<03:32, 17.69s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 229/240 [1:11:55<03:32, 19.35s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 230/240 [1:12:08<02:57, 17.70s/it]                                                    96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 230/240 [1:12:08<02:57, 17.70s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 231/240 [1:12:33<02:56, 19.63s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 232/240 [1:12:46<02:23, 17.92s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 233/240 [1:13:10<02:16, 19.54s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 234/240 [1:13:24<01:47, 17.86s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 235/240 [1:13:47<01:38, 19.61s/it]                                                    98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 235/240 [1:13:47<01:38, 19.61s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 236/240 [1:14:01<01:11, 17.87s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 237/240 [1:14:23<00:57, 19.16s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 238/240 [1:14:37<00:35, 17.53s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 239/240 [1:15:01<00:19, 19.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [1:15:15<00:00, 17.89s/it]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [1:15:15<00:00, 17.89s/it]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [1:15:15<00:00, 17.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [1:15:15<00:00, 18.82s/it]
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 25.00, Step: 200, Train Loss: N/A, Eval Loss: 0.8336045145988464, LR: N/A
{'eval_loss': 0.8336045145988464, 'eval_runtime': 12.8461, 'eval_samples_per_second': 14.946, 'eval_steps_per_second': 0.934, 'epoch': 25.0}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 25.62, Step: 205, Train Loss: N/A, Eval Loss: N/A, LR: 1.590909090909091e-05
{'loss': 0.6508, 'learning_rate': 1.590909090909091e-05, 'epoch': 25.62}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 26.25, Step: 210, Train Loss: N/A, Eval Loss: N/A, LR: 1.3636363636363637e-05
{'loss': 0.5727, 'learning_rate': 1.3636363636363637e-05, 'epoch': 26.25}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 26.88, Step: 215, Train Loss: N/A, Eval Loss: N/A, LR: 1.1363636363636365e-05
{'loss': 0.6418, 'learning_rate': 1.1363636363636365e-05, 'epoch': 26.88}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 27.50, Step: 220, Train Loss: N/A, Eval Loss: N/A, LR: 9.090909090909091e-06
{'loss': 0.5466, 'learning_rate': 9.090909090909091e-06, 'epoch': 27.5}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 28.12, Step: 225, Train Loss: N/A, Eval Loss: N/A, LR: 6.818181818181818e-06
{'loss': 0.6571, 'learning_rate': 6.818181818181818e-06, 'epoch': 28.12}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 28.75, Step: 230, Train Loss: N/A, Eval Loss: N/A, LR: 4.5454545454545455e-06
{'loss': 0.5779, 'learning_rate': 4.5454545454545455e-06, 'epoch': 28.75}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 29.38, Step: 235, Train Loss: N/A, Eval Loss: N/A, LR: 2.2727272727272728e-06
{'loss': 0.646, 'learning_rate': 2.2727272727272728e-06, 'epoch': 29.38}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 30.00, Step: 240, Train Loss: N/A, Eval Loss: N/A, LR: 0.0
{'loss': 0.5642, 'learning_rate': 0.0, 'epoch': 30.0}
ðŸ“Š [è®­ç»ƒæ—¥å¿—] Epoch: 30.00, Step: 240, Train Loss: 0.8573504209518432, Eval Loss: N/A, LR: N/A
{'train_runtime': 4515.9628, 'train_samples_per_second': 6.803, 'train_steps_per_second': 0.053, 'train_loss': 0.8573504209518432, 'epoch': 30.0}

 If there's a warning about missing keys above, please disregard :)
