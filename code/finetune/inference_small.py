#!/usr/bin/env python3
import sys
import fire
import torch
import json
import os
from tqdm import tqdm

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'
os.environ["HF_HOME"] = "/root/autodl-tmp/nku/DEALRec-main/models_cache"
os.environ["TRANSFORMERS_CACHE"] = "/root/autodl-tmp/nku/DEALRec-main/models_cache"

from peft import PeftModel
from transformers import GenerationConfig, LlamaTokenizer, LlamaForCausalLM
from torch.utils.data import Dataset, DataLoader

# è®¾å¤‡è®¾ç½®
device = "cuda" if torch.cuda.is_available() else "cpu"

class TestData(Dataset):
    def __init__(self, test_data_path):
        super().__init__()
        print(f"ğŸ“– åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
        
        with open(test_data_path, 'r') as f:
            test_data = json.load(f)
        
        self.instructions = [item['instruction'] for item in test_data]
        self.inputs = [item['input'] for item in test_data]
        self.golds = [item['output'] for item in test_data]
        
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.instructions)} ä¸ªæ ·æœ¬")

    def __len__(self):
        return len(self.instructions)
    
    def __getitem__(self, idx):
        return (self.instructions[idx], self.inputs[idx], self.golds[idx])

class TestCollator(object):
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token_id = 0
        self.tokenizer.padding_side = "left"

    def __call__(self, batch):
        instructions = [item[0] for item in batch]
        inputs = [item[1] for item in batch]
        golds = [item[2] for item in batch]
        
        prompts = [generate_prompt(instruction, input) for instruction, input in zip(instructions, inputs)]
        inputs = self.tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=256)
        
        return (inputs, golds)

def generate_prompt(instruction, input=None):
    if input:
        return f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
"""
    else:
        return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:
"""

def main(
    base_model: str = "/root/autodl-tmp/nku/DEALRec-main/models_cache/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75",
    lora_weights: str = "./model/games/2023_1024",
    dataset: str = "games",
    result_json_data: str = "./results/games/2023_1024_small.json",
    batch_size: int = 4,
    beam_size: int = 2,
    max_new_tokens: int = 96,
    use_small_test: bool = True
):
    print("ğŸš€ å¼€å§‹å¿«é€Ÿæ¨ç†æµ‹è¯•...")
    print(f"åŸºç¡€æ¨¡å‹: {base_model}")
    print(f"LoRAæƒé‡: {lora_weights}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # ç¡®å®šæµ‹è¯•æ•°æ®è·¯å¾„
    if use_small_test:
        test_data_path = f"data/{dataset}/test/test_small.json"
        print(f"ğŸ“ ä½¿ç”¨å°æµ‹è¯•é›†: {test_data_path}")
    else:
        test_data_path = f"data/{dataset}/test/test.json"
        print(f"ğŸ“ ä½¿ç”¨å®Œæ•´æµ‹è¯•é›†: {test_data_path}")
    
    # åŠ è½½tokenizer
    print("ğŸ”¤ åŠ è½½tokenizer...")
    tokenizer = LlamaTokenizer.from_pretrained(base_model)
    tokenizer.pad_token_id = 0
    tokenizer.padding_side = "left"

    # åŠ è½½æ¨¡å‹
    print("ğŸ¤– åŠ è½½åŸºç¡€æ¨¡å‹...")
    model = LlamaForCausalLM.from_pretrained(
        base_model,
        load_in_8bit=True,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    
    # åŠ è½½LoRAæƒé‡
    if os.path.exists(lora_weights):
        print(f"ğŸ¯ åŠ è½½LoRAæƒé‡: {lora_weights}")
        model = PeftModel.from_pretrained(
            model,
            lora_weights,
            torch_dtype=torch.float16,
            device_map="auto",
            is_trainable=False
        )
    else:
        print(f"âš ï¸ LoRAæƒé‡æœªæ‰¾åˆ°: {lora_weights}")
        return

    # è®¾ç½®æ¨¡å‹é…ç½®
    model.config.pad_token_id = tokenizer.pad_token_id = 0
    model.config.bos_token_id = 1
    model.config.eos_token_id = 2
    model.eval()

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    # æ¨ç†å‡½æ•°
    def evaluate(inputs, num_beams=2, max_new_tokens=96):
        generation_config = GenerationConfig(
            num_beams=num_beams,
            num_return_sequences=1,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
            temperature=0.2,
            repetition_penalty=1.05,
            early_stopping=True,
        )
        
        with torch.no_grad():
            generation_output = model.generate(
                **inputs,
                generation_config=generation_config,
                return_dict_in_generate=True,
                output_scores=True,
                max_new_tokens=max_new_tokens,
            )
        
        sequences = generation_output.sequences
        output = tokenizer.batch_decode(sequences, skip_special_tokens=True)
        
        processed_output = []
        for text in output:
            if 'Response:\n' in text:
                response = text.split('Response:\n')[-1].strip()
            elif 'Response:' in text:
                response = text.split('Response:')[-1].strip()
            elif '### Response:' in text:
                response = text.split('### Response:')[-1].strip()
            else:
                lines = text.strip().split('\n')
                response_lines = []
                skip_prompt = False
                for line in lines:
                    if 'Below is an instruction' in line or '### Instruction:' in line or '### Input:' in line:
                        skip_prompt = True
                        continue
                    if '### Response:' in line:
                        skip_prompt = False
                        continue
                    if not skip_prompt and line.strip():
                        response_lines.append(line.strip())
                
                response = ' '.join(response_lines[-3:]) if response_lines else text[-100:]
            
            response = response.replace('###', '').replace('Instruction:', '').replace('Input:', '').strip()
            processed_output.append(response)
        
        return processed_output

    # å‡†å¤‡æ•°æ®
    test_data = TestData(test_data_path)
    collator = TestCollator(tokenizer)
    test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collator, shuffle=False)

    # æ‰¹é‡æ¨ç†
    print(f"ğŸ”® å¼€å§‹æ¨ç†ï¼Œå…± {len(test_data)} ä¸ªæ ·æœ¬...")
    all_predictions = []
    all_targets = []
    
    for i, batch in enumerate(tqdm(test_loader, desc="æ¨ç†è¿›è¡Œä¸­")):
        inputs, targets = batch
        inputs = inputs.to(device)
        
        try:
            predictions = evaluate(inputs, num_beams=beam_size, max_new_tokens=max_new_tokens)
            all_predictions.extend(predictions)
            all_targets.extend(targets)
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœ
            if i == 0:
                print("\nğŸ“‹ å‰3ä¸ªæ¨ç†ç¤ºä¾‹:")
                for j in range(min(3, len(predictions))):
                    print(f"æ ·æœ¬ {j+1}:")
                    print(f"  é¢„æµ‹: {predictions[j][:100]}...")
                    print(f"  çœŸå®: {targets[j][:100]}...")
                    print()
                    
        except Exception as e:
            print(f"âŒ æ¨ç†å‡ºé”™: {e}")
            break

    # ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜æ¨ç†ç»“æœ...")
    results = []
    for pred, target in zip(all_predictions, all_targets):
        results.append({
            'predict': pred,
            'output': target
        })
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(os.path.dirname(result_json_data), exist_ok=True)
    
    with open(result_json_data, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æ¨ç†å®Œæˆï¼")
    print(f"ğŸ“Š å¤„ç†æ ·æœ¬æ•°: {len(results)}")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {result_json_data}")
    
    return results

if __name__ == "__main__":
    fire.Fire(main) 