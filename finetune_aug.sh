#!/bin/bash

# å¢å¼ºæ•°æ®é›†å¾®è°ƒè„šæœ¬ - 1932æ ·æœ¬è®­ç»ƒ
echo "ğŸ”§ å¼€å§‹å¢å¼ºæ•°æ®é›†å®Œæ•´æµç¨‹ (1932æ ·æœ¬)"
echo "åŒ…å«ï¼šå¾®è°ƒè®­ç»ƒ -> å°è§„æ¨¡æ¨ç† -> å°è§„æ¨¡è¯„ä¼°"

# è®¾ç½®è·¯å¾„
MODEL_PATH="/root/autodl-tmp/nku/DEALRec-main/models_cache/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75"
TRAIN_DATA_PATH="code/finetune/data/games/train/train-1932.json"
VAL_DATA_PATH="code/finetune/data/games/valid/valid-365.json"
OUTPUT_DIR="code/finetune/model/games/2023_1932_aug"
RESULT_FILE="code/finetune/results/games/2023_1932_aug_small.json"

# åˆ›å»ºè¾“å‡ºç›®å½•å’Œæ—¥å¿—ç›®å½•
mkdir -p "$OUTPUT_DIR"
mkdir -p "code/finetune/results/games"
LOG_DIR="code/finetune/logs/games/2023_1932_aug"
mkdir -p "$LOG_DIR"

echo "==== æ£€æŸ¥æ¨¡å‹çŠ¶æ€ ===="
echo "æ¨¡å‹ç›®å½•: $OUTPUT_DIR"
echo "ç»“æœæ–‡ä»¶: $RESULT_FILE"

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
if [ -f "$OUTPUT_DIR/adapter_model.bin" ]; then
    echo "âœ… å‘ç°å·²è®­ç»ƒçš„æ¨¡å‹ï¼"
    echo "ğŸ“ ä½ç½®: $OUTPUT_DIR"
    
    # æ£€æŸ¥æ¨ç†ç»“æœæ˜¯å¦å·²å­˜åœ¨
    if [ -f "$RESULT_FILE" ]; then
        echo "âœ… æ¨ç†ç»“æœä¹Ÿå·²å­˜åœ¨ï¼Œè·³è¿‡æ•´ä¸ªæµç¨‹"
        echo "ğŸ“„ ç»“æœæ–‡ä»¶: $RESULT_FILE"
        echo "å¦‚éœ€é‡æ–°è®­ç»ƒï¼Œè¯·åˆ é™¤æ¨¡å‹ç›®å½•: rm -rf $OUTPUT_DIR"
        exit 0
    else
        echo "âŒ æ¨ç†ç»“æœä¸å­˜åœ¨ï¼Œç›´æ¥è¿›è¡Œæ¨ç†"
        skip_training=true
    fi
else
    echo "âŒ æœªå‘ç°å·²è®­ç»ƒæ¨¡å‹ï¼Œå¼€å§‹ä»å¤´è®­ç»ƒ"
    skip_training=false
fi

# æ ¹æ®æ£€æŸ¥ç»“æœå†³å®šæ˜¯å¦è®­ç»ƒ
if [ "$skip_training" = false ]; then
    echo "==== å¼€å§‹å¾®è°ƒè®­ç»ƒ ===="
    echo "ğŸ”§ è®­ç»ƒå‚æ•°ï¼ˆä¸full_finetune_smart.shä¿æŒä¸€è‡´ï¼‰ï¼š"
    echo "   - åŸºç¡€æ¨¡å‹: LLaMA-7B"
    echo "   - è®­ç»ƒæ•°æ®: $TRAIN_DATA_PATH"
    echo "   - éªŒè¯æ•°æ®: $VAL_DATA_PATH"
    echo "   - è®­ç»ƒæ ·æœ¬: 1932ä¸ª"
    echo "   - éªŒè¯æ ·æœ¬: 365ä¸ª"
    echo "   - æ‰¹æ¬¡å¤§å°: 128"
    echo "   - å¾®æ‰¹æ¬¡å¤§å°: 16"
    echo "   - è®­ç»ƒè½®æ•°: 30"
    echo "   - å­¦ä¹ ç‡: 1e-4"
    echo "   - LoRA rank: 8"
    echo "   - LoRA alpha: 16"

    # å¼€å§‹è®­ç»ƒ - ä½¿ç”¨ä¸full_finetune_smart.shå®Œå…¨ç›¸åŒçš„å‚æ•°
    CUDA_VISIBLE_DEVICES=0 python code/finetune/finetune.py \
        --base_model="$MODEL_PATH" \
        --train_data_path="$TRAIN_DATA_PATH" \
        --val_data_path="$VAL_DATA_PATH" \
        --output_dir="$OUTPUT_DIR" \
        --batch_size=128 \
        --micro_batch_size=16 \
        --num_epochs=30 \
        --learning_rate=1e-4 \
        --cutoff_len=512 \
        --lora_r=8 \
        --lora_alpha=16 \
        --lora_dropout=0.05 \
        --lora_target_modules='[q_proj,v_proj,k_proj,o_proj]' \
        --train_on_inputs \
        --group_by_length \
        --seed=2023 \
        --sample=-1 \
        --log_file="$LOG_DIR/training_detailed.csv" \
        2>&1 | tee "$LOG_DIR/training_console.log"

    echo "==== å¾®è°ƒè®­ç»ƒå®Œæˆ ===="
else
    echo "==== è·³è¿‡è®­ç»ƒï¼Œä½¿ç”¨å·²å­˜åœ¨æ¨¡å‹ ===="
fi

echo "==== å¼€å§‹å°è§„æ¨¡æ¨ç† ===="

# å°è§„æ¨¡æ¨ç† (ä½¿ç”¨inference_small.py)
CUDA_VISIBLE_DEVICES=0 python code/finetune/inference_small.py \
    --base_model="$MODEL_PATH" \
    --lora_weights="$OUTPUT_DIR" \
    --result_json_data="$RESULT_FILE" \
    --dataset="games" \
    --batch_size=4 \
    --beam_size=2 \
    --max_new_tokens=96 \
    --use_small_test=True

echo "==== æ¨ç†å®Œæˆï¼Œå¼€å§‹å°è§„æ¨¡è¯„ä¼° ===="

# å°è§„æ¨¡è¯„ä¼°
echo "å½“å‰å·¥ä½œç›®å½•: $(pwd)"
CUDA_VISIBLE_DEVICES=0 python code/finetune/evaluate_small.py \
    --result_file="$RESULT_FILE" \
    --dataset="games" \
    --batch_size=4

echo "==== å®Œæ•´æµç¨‹å®Œæˆï¼===="
echo "ğŸ”§ æ¨¡å‹ä¿å­˜ä½ç½®: $OUTPUT_DIR"
echo "ğŸ”§ ç»“æœä¿å­˜ä½ç½®: $RESULT_FILE"
echo "ğŸ”§ æ—¥å¿—ä¿å­˜ä½ç½®: $LOG_DIR"
echo "=========================================="

echo "ğŸ‰ å¢å¼ºæ•°æ®é›†å®éªŒå®Œæˆï¼(1932æ ·æœ¬)" 